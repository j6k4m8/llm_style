{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "040dc41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from llm_style.formality_dataset import FormalityDataset, PavlickFormalityDataset\n",
    "from llm_style.domain_dataset import DomainDataset, DomainType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bc68eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DomainDataset(mode=\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34f0fbd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df0163218b7748ea84f39d20d69385f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"microsoft/phi-4\",\n",
    "    model_kwargs={\"torch_dtype\": \"auto\"},\n",
    "    device_map=\"auto\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0216a079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Get device for computation\n",
    "device = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Extract texts and labels from the dataset\n",
    "texts = []\n",
    "labels = []\n",
    "for i in range(0, 250):\n",
    "    # if i >= len(dataset):\n",
    "    #     break\n",
    "    sample = dataset[i]\n",
    "    if \"text\" not in sample or \"label\" not in sample:\n",
    "        print(f\"Skipping index {i} due to missing fields.\")\n",
    "        continue\n",
    "    texts.append(sample[\"text\"])\n",
    "    labels.append(sample[\"label\"])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Define a function to extract features from text\n",
    "def extract_features(model, tokenizer, texts, layer_idx):\n",
    "    # Move model to the correct device if not already there\n",
    "    # model_device = next(model.parameters()).device\n",
    "    # if model_device != device:\n",
    "    #     model = model.to(device)\n",
    "\n",
    "    features = []\n",
    "    batch_size = 16  # Adjust based on your GPU/memory\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i : i + batch_size]\n",
    "        # Tokenize the texts\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Get model outputs with no gradient computation\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "        # Extract hidden states from the specified layer\n",
    "        # For phi-4 model, access the proper hidden states format\n",
    "        if hasattr(outputs, \"hidden_states\"):\n",
    "            hidden_states = outputs.hidden_states[layer_idx]\n",
    "        else:\n",
    "            # If hidden_states is a tuple, access it directly\n",
    "            hidden_states = outputs[2][layer_idx]\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "\n",
    "        # Average across the token dimension to get a fixed-size representation\n",
    "        batch_features = hidden_states.mean(dim=1)\n",
    "        batch_features_cpu = batch_features.cpu().numpy()\n",
    "        features.append(batch_features_cpu)\n",
    "\n",
    "    return np.vstack(features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9555e928",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentBottleneck(nn.Module):\n",
    "    def __init__(self, dim_model: int, dim_latent: int):\n",
    "        super().__init__()\n",
    "        self.down = nn.Linear(dim_model, dim_latent)\n",
    "        self.up = nn.Linear(dim_latent, dim_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.up(self.down(x))\n",
    "\n",
    "    def forward_latent(self, x):\n",
    "        return self.down(x)\n",
    "\n",
    "class LatentClassifier(nn.Module):\n",
    "    def __init__(self, bottleneck: LatentBottleneck, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.bottleneck = bottleneck\n",
    "        self.classifier = nn.Linear(bottleneck.down.out_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.bottleneck.forward_latent(x)\n",
    "        return self.classifier(latent)\n",
    "\n",
    "class ResidualBottleneckWrapper(nn.Module):\n",
    "    def __init__(self, layer: nn.Module, dim_model: int, dim_latent: int):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.bottleneck = LatentBottleneck(dim_model, dim_latent)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        bottleneck_out = self.bottleneck(x)\n",
    "        x_modified = x + bottleneck_out\n",
    "        return self.layer(x_modified, *args, **kwargs)\n",
    "\n",
    "# Example: bottleneck from original 5120\n",
    "bottleneck = LatentBottleneck(dim_model=5120, dim_latent=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b875938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers except the bottleneck\n",
    "for param in pipeline.model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in bottleneck.parameters():\n",
    "    param.requires_grad = True\n",
    "# Insert the bottleneck into the model\n",
    "\n",
    "# Wrap layer 21 in a residual bottleneck\n",
    "original_layer = pipeline.model.model.layers[21]\n",
    "dim_model = 5120\n",
    "dim_latent = 6\n",
    "wrapped_layer = ResidualBottleneckWrapper(original_layer, dim_model, dim_latent)\n",
    "wrapped_layer = wrapped_layer.to_empty(device=device)\n",
    "\n",
    "# Replace in-place\n",
    "pipeline.model.model.layers[21] = wrapped_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c559cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: model.layers.21.bottleneck.down.weight, Params: 30720\n",
      "Layer: model.layers.21.bottleneck.down.bias, Params: 6\n",
      "Layer: model.layers.21.bottleneck.up.weight, Params: 30720\n",
      "Layer: model.layers.21.bottleneck.up.bias, Params: 5120\n"
     ]
    }
   ],
   "source": [
    "# Print all the layers and the count of their params:\n",
    "def print_layers(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"Layer: {name}, Params: {param.numel()}\")\n",
    "print_layers(pipeline.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376f9aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll train the bottleneck to one-hot the label\n",
    "def train_latent_classifier(\n",
    "    model: nn.Module,\n",
    "    llm: nn.Module,\n",
    "    tokenizer,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    layer_idx=21,\n",
    "    epochs=20,\n",
    "    batch_size=16,\n",
    "):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1 * 10**(-3.5))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            batch_texts = X_train[i : i + batch_size]\n",
    "            batch_labels = y_train[i : i + batch_size]\n",
    "\n",
    "            # Tokenize\n",
    "            inputs = tokenizer(\n",
    "                batch_texts,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "            ).to(device)\n",
    "\n",
    "            # Forward through frozen LLM to get hidden states\n",
    "            with torch.no_grad():\n",
    "                outputs = llm(**inputs, output_hidden_states=True)\n",
    "                hidden_states = outputs.hidden_states[layer_idx]\n",
    "                x = hidden_states.mean(dim=1).to(torch.float32).to(device)\n",
    "\n",
    "            # Forward through latent classifier\n",
    "            labels_tensor = torch.tensor(batch_labels, device=device)\n",
    "            preds = model(x)\n",
    "            loss = criterion(preds, labels_tensor)\n",
    "\n",
    "            # Clip gradients to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            # Optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}, Batch {i//batch_size+1}, Loss: {loss.item():.4f}\")\n",
    "        # Save the model at the end of each epoch\n",
    "        torch.save(model.state_dict(), f\"latent_classifier_epoch_{epoch+1}.pt\")\n",
    "        print(f\"Epoch {epoch+1} completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e1caad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1, Loss: 1.5235\n",
      "Epoch 1, Batch 2, Loss: 1.2272\n",
      "Epoch 1, Batch 3, Loss: 1.4871\n",
      "Epoch 1, Batch 4, Loss: 1.1445\n",
      "Epoch 1, Batch 5, Loss: 1.1023\n",
      "Epoch 1, Batch 6, Loss: 1.3120\n",
      "Epoch 1, Batch 7, Loss: 0.9299\n",
      "Epoch 1 completed.\n",
      "Epoch 2, Batch 1, Loss: 0.9969\n",
      "Epoch 2, Batch 2, Loss: 0.9220\n",
      "Epoch 2, Batch 3, Loss: 1.2098\n",
      "Epoch 2, Batch 4, Loss: 0.7934\n",
      "Epoch 2, Batch 5, Loss: 0.8896\n",
      "Epoch 2, Batch 6, Loss: 1.0152\n",
      "Epoch 2, Batch 7, Loss: 0.6803\n",
      "Epoch 2 completed.\n",
      "Epoch 3, Batch 1, Loss: 0.7945\n",
      "Epoch 3, Batch 2, Loss: 0.6719\n",
      "Epoch 3, Batch 3, Loss: 0.9211\n",
      "Epoch 3, Batch 4, Loss: 0.7461\n",
      "Epoch 3, Batch 5, Loss: 0.7406\n",
      "Epoch 3, Batch 6, Loss: 0.8359\n",
      "Epoch 3, Batch 7, Loss: 0.3902\n",
      "Epoch 3 completed.\n",
      "Epoch 4, Batch 1, Loss: 0.6108\n",
      "Epoch 4, Batch 2, Loss: 0.5911\n",
      "Epoch 4, Batch 3, Loss: 0.8244\n",
      "Epoch 4, Batch 4, Loss: 0.5896\n",
      "Epoch 4, Batch 5, Loss: 0.7070\n",
      "Epoch 4, Batch 6, Loss: 0.7662\n",
      "Epoch 4, Batch 7, Loss: 0.2326\n",
      "Epoch 4 completed.\n",
      "Epoch 5, Batch 1, Loss: 0.5320\n",
      "Epoch 5, Batch 2, Loss: 0.5188\n",
      "Epoch 5, Batch 3, Loss: 0.6936\n",
      "Epoch 5, Batch 4, Loss: 0.5207\n",
      "Epoch 5, Batch 5, Loss: 0.5601\n",
      "Epoch 5, Batch 6, Loss: 0.6391\n",
      "Epoch 5, Batch 7, Loss: 0.2094\n",
      "Epoch 5 completed.\n",
      "Epoch 6, Batch 1, Loss: 0.4647\n",
      "Epoch 6, Batch 2, Loss: 0.4593\n",
      "Epoch 6, Batch 3, Loss: 0.6218\n",
      "Epoch 6, Batch 4, Loss: 0.4693\n",
      "Epoch 6, Batch 5, Loss: 0.5074\n",
      "Epoch 6, Batch 6, Loss: 0.5647\n",
      "Epoch 6, Batch 7, Loss: 0.1456\n",
      "Epoch 6 completed.\n",
      "Epoch 7, Batch 1, Loss: 0.4033\n",
      "Epoch 7, Batch 2, Loss: 0.4211\n",
      "Epoch 7, Batch 3, Loss: 0.5721\n",
      "Epoch 7, Batch 4, Loss: 0.4023\n",
      "Epoch 7, Batch 5, Loss: 0.4680\n",
      "Epoch 7, Batch 6, Loss: 0.4973\n",
      "Epoch 7, Batch 7, Loss: 0.1208\n",
      "Epoch 7 completed.\n",
      "Epoch 8, Batch 1, Loss: 0.3638\n",
      "Epoch 8, Batch 2, Loss: 0.3803\n",
      "Epoch 8, Batch 3, Loss: 0.5086\n",
      "Epoch 8, Batch 4, Loss: 0.3670\n",
      "Epoch 8, Batch 5, Loss: 0.4092\n",
      "Epoch 8, Batch 6, Loss: 0.4423\n",
      "Epoch 8, Batch 7, Loss: 0.1069\n",
      "Epoch 8 completed.\n",
      "Epoch 9, Batch 1, Loss: 0.3340\n",
      "Epoch 9, Batch 2, Loss: 0.3425\n",
      "Epoch 9, Batch 3, Loss: 0.4572\n",
      "Epoch 9, Batch 4, Loss: 0.3149\n",
      "Epoch 9, Batch 5, Loss: 0.3771\n",
      "Epoch 9, Batch 6, Loss: 0.4051\n",
      "Epoch 9, Batch 7, Loss: 0.0876\n",
      "Epoch 9 completed.\n",
      "Epoch 10, Batch 1, Loss: 0.3034\n",
      "Epoch 10, Batch 2, Loss: 0.3122\n",
      "Epoch 10, Batch 3, Loss: 0.4164\n",
      "Epoch 10, Batch 4, Loss: 0.2823\n",
      "Epoch 10, Batch 5, Loss: 0.3416\n",
      "Epoch 10, Batch 6, Loss: 0.3608\n",
      "Epoch 10, Batch 7, Loss: 0.0768\n",
      "Epoch 10 completed.\n",
      "Epoch 11, Batch 1, Loss: 0.2800\n",
      "Epoch 11, Batch 2, Loss: 0.2850\n",
      "Epoch 11, Batch 3, Loss: 0.3789\n",
      "Epoch 11, Batch 4, Loss: 0.2527\n",
      "Epoch 11, Batch 5, Loss: 0.3092\n",
      "Epoch 11, Batch 6, Loss: 0.3287\n",
      "Epoch 11, Batch 7, Loss: 0.0668\n",
      "Epoch 11 completed.\n",
      "Epoch 12, Batch 1, Loss: 0.2577\n",
      "Epoch 12, Batch 2, Loss: 0.2602\n",
      "Epoch 12, Batch 3, Loss: 0.3455\n",
      "Epoch 12, Batch 4, Loss: 0.2242\n",
      "Epoch 12, Batch 5, Loss: 0.2832\n",
      "Epoch 12, Batch 6, Loss: 0.3015\n",
      "Epoch 12, Batch 7, Loss: 0.0585\n",
      "Epoch 12 completed.\n",
      "Epoch 13, Batch 1, Loss: 0.2356\n",
      "Epoch 13, Batch 2, Loss: 0.2376\n",
      "Epoch 13, Batch 3, Loss: 0.3134\n",
      "Epoch 13, Batch 4, Loss: 0.2056\n",
      "Epoch 13, Batch 5, Loss: 0.2580\n",
      "Epoch 13, Batch 6, Loss: 0.2754\n",
      "Epoch 13, Batch 7, Loss: 0.0520\n",
      "Epoch 13 completed.\n",
      "Epoch 14, Batch 1, Loss: 0.2166\n",
      "Epoch 14, Batch 2, Loss: 0.2177\n",
      "Epoch 14, Batch 3, Loss: 0.2872\n",
      "Epoch 14, Batch 4, Loss: 0.1839\n",
      "Epoch 14, Batch 5, Loss: 0.2358\n",
      "Epoch 14, Batch 6, Loss: 0.2533\n",
      "Epoch 14, Batch 7, Loss: 0.0465\n",
      "Epoch 14 completed.\n",
      "Epoch 15, Batch 1, Loss: 0.1997\n",
      "Epoch 15, Batch 2, Loss: 0.1994\n",
      "Epoch 15, Batch 3, Loss: 0.2624\n",
      "Epoch 15, Batch 4, Loss: 0.1682\n",
      "Epoch 15, Batch 5, Loss: 0.2153\n",
      "Epoch 15, Batch 6, Loss: 0.2329\n",
      "Epoch 15, Batch 7, Loss: 0.0420\n",
      "Epoch 15 completed.\n",
      "Epoch 16, Batch 1, Loss: 0.1839\n",
      "Epoch 16, Batch 2, Loss: 0.1828\n",
      "Epoch 16, Batch 3, Loss: 0.2405\n",
      "Epoch 16, Batch 4, Loss: 0.1533\n",
      "Epoch 16, Batch 5, Loss: 0.1974\n",
      "Epoch 16, Batch 6, Loss: 0.2152\n",
      "Epoch 16, Batch 7, Loss: 0.0380\n",
      "Epoch 16 completed.\n",
      "Epoch 17, Batch 1, Loss: 0.1699\n",
      "Epoch 17, Batch 2, Loss: 0.1678\n",
      "Epoch 17, Batch 3, Loss: 0.2209\n",
      "Epoch 17, Batch 4, Loss: 0.1402\n",
      "Epoch 17, Batch 5, Loss: 0.1810\n",
      "Epoch 17, Batch 6, Loss: 0.1986\n",
      "Epoch 17, Batch 7, Loss: 0.0346\n",
      "Epoch 17 completed.\n",
      "Epoch 18, Batch 1, Loss: 0.1574\n",
      "Epoch 18, Batch 2, Loss: 0.1541\n",
      "Epoch 18, Batch 3, Loss: 0.2033\n",
      "Epoch 18, Batch 4, Loss: 0.1288\n",
      "Epoch 18, Batch 5, Loss: 0.1663\n",
      "Epoch 18, Batch 6, Loss: 0.1836\n",
      "Epoch 18, Batch 7, Loss: 0.0316\n",
      "Epoch 18 completed.\n",
      "Epoch 19, Batch 1, Loss: 0.1459\n",
      "Epoch 19, Batch 2, Loss: 0.1417\n",
      "Epoch 19, Batch 3, Loss: 0.1876\n",
      "Epoch 19, Batch 4, Loss: 0.1184\n",
      "Epoch 19, Batch 5, Loss: 0.1532\n",
      "Epoch 19, Batch 6, Loss: 0.1701\n",
      "Epoch 19, Batch 7, Loss: 0.0290\n",
      "Epoch 19 completed.\n",
      "Epoch 20, Batch 1, Loss: 0.1354\n",
      "Epoch 20, Batch 2, Loss: 0.1306\n",
      "Epoch 20, Batch 3, Loss: 0.1732\n",
      "Epoch 20, Batch 4, Loss: 0.1094\n",
      "Epoch 20, Batch 5, Loss: 0.1414\n",
      "Epoch 20, Batch 6, Loss: 0.1577\n",
      "Epoch 20, Batch 7, Loss: 0.0267\n",
      "Epoch 20 completed.\n"
     ]
    }
   ],
   "source": [
    "# Train the bottleneck\n",
    "latent_classifier = LatentClassifier(bottleneck, num_classes=len(set(y_train))).to(device)\n",
    "train_latent_classifier(\n",
    "    latent_classifier,\n",
    "    pipeline.model,\n",
    "    pipeline.tokenizer,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    layer_idx=21,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c997db20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_domain(text: str, llm, tokenizer, classifier, layer_idx=21):\n",
    "    classifier.eval()\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = llm(**inputs, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states[layer_idx]\n",
    "        x = hidden_states.mean(dim=1).to(torch.float32).to(device)\n",
    "\n",
    "        logits = classifier(x)\n",
    "        predicted_class = logits.argmax(dim=1).item()\n",
    "\n",
    "    return DomainType.from_int(predicted_class), logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8124136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted domain class: blog\n"
     ]
    }
   ],
   "source": [
    "text = \"I'd try unplugging your modem first to see if that's the problem.\"\n",
    "pred, _ = predict_domain(text, pipeline.model, pipeline.tokenizer, latent_classifier, layer_idx=21)\n",
    "print(f\"Predicted domain class: {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "733ef632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted domain class: blog\n"
     ]
    }
   ],
   "source": [
    "text = \"When I first set out on this vacation, I was worried my shoes wouldn't last.\"\n",
    "pred, _ = predict_domain(text, pipeline.model, pipeline.tokenizer, latent_classifier, layer_idx=21)\n",
    "print(f\"Predicted domain class: {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ebf4c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted domain class: email\n"
     ]
    }
   ],
   "source": [
    "text = \"You need to send me the report by the end of the day.\"\n",
    "pred, _ = predict_domain(text, pipeline.model, pipeline.tokenizer, latent_classifier, layer_idx=21)\n",
    "print(f\"Predicted domain class: {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f79946e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted domain class: news\n"
     ]
    }
   ],
   "source": [
    "text = \"In breaking news, a major earthquake has struck the city, causing widespread damage and panic.\"\n",
    "pred, _ = predict_domain(text, pipeline.model, pipeline.tokenizer, latent_classifier, layer_idx=21)\n",
    "print(f\"Predicted domain class: {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4313707b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, classes):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "    plt.xticks(np.arange(len(classes)), classes, rotation=45)\n",
    "    plt.yticks(np.arange(len(classes)), classes)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5874e702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "y_pred = []\n",
    "for text in X_test:\n",
    "    pred, _ = predict_domain(text, pipeline.model, pipeline.tokenizer, latent_classifier, layer_idx=21)\n",
    "    y_pred.append(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6f97c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5400\n"
     ]
    }
   ],
   "source": [
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(y_test, [DomainType.as_int(y) for y in y_pred], [\n",
    "\"ANSWERS\",\n",
    "\"BLOG\",\n",
    "\"EMAIL\",\n",
    "\"NEWS\",\n",
    "])\n",
    "# Print accuracy\n",
    "accuracy = accuracy_score(y_test, [DomainType.as_int(y) for y in y_pred])\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713c44cb",
   "metadata": {},
   "source": [
    "## Content Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a74cc114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import evaluate\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "from llm_style.content_dataset import CNNContentDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47c979d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in CNN dataset: 287113\n",
      "Sample 0: {'article': 'LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won\\'t cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don\\'t plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don\\'t think I\\'ll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he\\'ll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I\\'ll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe\\'s earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say \\'kid star goes off the rails,\\'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films.  Watch I-Reporter give her review of Potter\\'s latest » . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer\\'s \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he\\'s legally an adult: \"I just think I\\'m going to be more sort of fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.', 'summary': \"Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\\nYoung actor says he has no plans to fritter his cash away .\\nRadcliffe's earnings from first five Potter films have been held in trust fund .\"}\n",
      "Sample 1: {'article': 'Editor\\'s note: In our Behind the Scenes series, CNN correspondents share their experiences in covering news and analyze the stories behind the events. Here, Soledad O\\'Brien takes users inside a jail where many of the inmates are mentally ill. An inmate housed on the \"forgotten floor,\" where many mentally ill inmates are housed in Miami before trial. MIAMI, Florida (CNN) -- The ninth floor of the Miami-Dade pretrial detention facility is dubbed the \"forgotten floor.\" Here, inmates with the most severe mental illnesses are incarcerated until they\\'re ready to appear in court. Most often, they face drug charges or charges of assaulting an officer --charges that Judge Steven Leifman says are usually \"avoidable felonies.\" He says the arrests often result from confrontations with police. Mentally ill people often won\\'t do what they\\'re told when police arrive on the scene -- confrontation seems to exacerbate their illness and they become more paranoid, delusional, and less likely to follow directions, according to Leifman. So, they end up on the ninth floor severely mentally disturbed, but not getting any real help because they\\'re in jail. We toured the jail with Leifman. He is well known in Miami as an advocate for justice and the mentally ill. Even though we were not exactly welcomed with open arms by the guards, we were given permission to shoot videotape and tour the floor.  Go inside the \\'forgotten floor\\' » . At first, it\\'s hard to determine where the people are. The prisoners are wearing sleeveless robes. Imagine cutting holes for arms and feet in a heavy wool sleeping bag -- that\\'s kind of what they look like. They\\'re designed to keep the mentally ill patients from injuring themselves. That\\'s also why they have no shoes, laces or mattresses. Leifman says about one-third of all people in Miami-Dade county jails are mentally ill. So, he says, the sheer volume is overwhelming the system, and the result is what we see on the ninth floor. Of course, it is a jail, so it\\'s not supposed to be warm and comforting, but the lights glare, the cells are tiny and it\\'s loud. We see two, sometimes three men -- sometimes in the robes, sometimes naked, lying or sitting in their cells. \"I am the son of the president. You need to get me out of here!\" one man shouts at me. He is absolutely serious, convinced that help is on the way -- if only he could reach the White House. Leifman tells me that these prisoner-patients will often circulate through the system, occasionally stabilizing in a mental hospital, only to return to jail to face their charges. It\\'s brutally unjust, in his mind, and he has become a strong advocate for changing things in Miami. Over a meal later, we talk about how things got this way for mental patients. Leifman says 200 years ago people were considered \"lunatics\" and they were locked up in jails even if they had no charges against them. They were just considered unfit to be in society. Over the years, he says, there was some public outcry, and the mentally ill were moved out of jails and into hospitals. But Leifman says many of these mental hospitals were so horrible they were shut down. Where did the patients go? Nowhere. The streets. They became, in many cases, the homeless, he says. They never got treatment. Leifman says in 1955 there were more than half a million people in state mental hospitals, and today that number has been reduced 90 percent, and 40,000 to 50,000 people are in mental hospitals. The judge says he\\'s working to change this. Starting in 2008, many inmates who would otherwise have been brought to the \"forgotten floor\"  will instead be sent to a new mental health facility -- the first step on a journey toward long-term treatment, not just punishment. Leifman says it\\'s not the complete answer, but it\\'s a start. Leifman says the best part is that it\\'s a win-win solution. The patients win, the families are relieved, and the state saves money by simply not cycling these prisoners through again and again. And, for Leifman, justice is served. E-mail to a friend .', 'summary': 'Mentally ill inmates in Miami are housed on the \"forgotten floor\"\\nJudge Steven Leifman says most are there as a result of \"avoidable felonies\"\\nWhile CNN tours facility, patient shouts: \"I am the son of the president\"\\nLeifman says the system is unjust and he\\'s fighting for change .'}\n",
      "Sample 2: {'article': 'MINNEAPOLIS, Minnesota (CNN) -- Drivers who were on the Minneapolis bridge when it collapsed told harrowing tales of survival. \"The whole bridge from one side of the Mississippi to the other just completely gave way, fell all the way down,\" survivor Gary Babineau told CNN. \"I probably had a 30-, 35-foot free fall. And there\\'s cars in the water, there\\'s cars on fire. The whole bridge is down.\" He said his back was injured but he determined he could move around. \"I realized there was a school bus right next to me, and me and a couple of other guys went over and started lifting the kids off the bridge. They were yelling, screaming, bleeding. I think there were some broken bones.\"  Watch a driver describe his narrow escape » . At home when he heard about the disaster, Dr. John Hink, an emergency room physician, jumped into his car and rushed to the scene in 15 minutes. He arrived at the south side of the bridge, stood on the riverbank and saw dozens of people lying dazed on an expansive deck. They were in the middle of the Mississippi River, which was churning fast, and he had no way of getting to them. He went to the north side, where there was easier access to people. Ambulances were also having a hard time driving down to the river to get closer to the scene. Working feverishly, volunteers, EMTs and other officials managed to get 55 people into ambulances in less than two hours. Occasionally, a pickup truck with a medic inside would drive to get an injured person and bring him back up even ground, Hink told CNN. The rescue effort was controlled and organized, he said; the opposite of the lightning-quick collapse. \"I could see the whole bridge as it was going down, as it was falling,\" Babineau said. \"It just gave a rumble real quick, and it all just gave way, and it just fell completely, all the way to the ground. And there was dust everywhere and it was just like everyone has been saying: It was just like out of the movies.\" Babineau said the rear of his pickup truck was dangling over the edge of a broken-off section of the bridge. He said several vehicles slid past him into the water. \"I stayed in my car for one or two seconds. I saw a couple cars fall,\" he said. \"So I stayed in my car until the cars quit falling for a second, then I got out real quick, ran in front of my truck -- because behind my truck was just a hole -- and I helped a woman off of the bridge with me. \"I just wanted off the bridge, and then I ran over to the school bus. I started grabbing kids and handing them down. It was just complete chaos.\" He said most of the children were crying or screaming. He and other rescuers set them on the ground and told them to run to the river bank, but a few needed to be carried because of their injuries.  See rescuers clamber over rubble » . Babineau said he had no rescue training. \"I just knew what I had to do at the moment.\" Melissa Hughes, 32, of Minneapolis, told The Associated Press that she was driving home when the western edge of the bridge collapsed under her. \"You know that free-fall feeling? I felt that twice,\" Hughes said. A pickup landed on top of her car, but she was not hurt. \"I had no idea there was a vehicle on my car,\" she told AP. \"It\\'s really very surreal.\" Babineau told the Minneapolis Star-Tribune: \"On the way down, I thought I was dead. I literally thought I was dead. \"My truck was completely face down, pointed toward the ground, and my truck got ripped in half. It was folded in half, and I can\\'t believe I\\'m alive.\"  See and hear eyewitness accounts » . Bernie Toivonen told CNN\\'s \"American Morning\" that his vehicle was on a part of the bridge that ended up tilted at a 45-degree angle. \"I knew the deck was going down, there was no question about it, and I thought I was going to die,\" he said. After the bridge settled and his car remained upright, \"I just put in park, turned the key off and said, \\'Oh, I\\'m alive,\\' \" he said. E-mail to a friend .', 'summary': 'NEW: \"I thought I was going to die,\" driver says .\\nMan says pickup truck was folded in half; he just has cut on face .\\nDriver: \"I probably had a 30-, 35-foot free fall\"\\nMinnesota bridge collapsed during rush hour Wednesday .'}\n",
      "Sample 3: {'article': 'WASHINGTON (CNN) -- Doctors removed five small polyps from President Bush\\'s colon on Saturday, and \"none appeared worrisome,\" a White House spokesman said. The polyps were removed and sent to the National Naval Medical Center in Bethesda, Maryland, for routine microscopic examination, spokesman Scott Stanzel said. Results are expected in two to three days. All were small, less than a centimeter [half an inch] in diameter, he said. Bush is in good humor, Stanzel said, and will resume his activities at Camp David. During the procedure Vice President Dick Cheney assumed presidential power. Bush reclaimed presidential power at 9:21 a.m. after about two hours. Doctors used \"monitored anesthesia care,\" Stanzel said, so the president was asleep, but not as deeply unconscious as with a true general anesthetic. He spoke to first lady Laura Bush -- who is in Midland, Texas, celebrating her mother\\'s birthday -- before and after the procedure, Stanzel said. Afterward, the president played with his Scottish terriers, Barney and Miss Beazley, Stanzel said. He planned to have lunch at Camp David and have briefings with National Security Adviser Stephen Hadley and White House Chief of Staff Josh Bolten, and planned to take a bicycle ride Saturday afternoon. Cheney, meanwhile, spent the morning at his home on Maryland\\'s eastern shore, reading and playing with his dogs, Stanzel said. Nothing occurred that required him to take official action as president before Bush reclaimed presidential power. The procedure was supervised by Dr. Richard Tubb, Bush\\'s physician, and conducted by a multidisciplinary team from the National Naval Medical Center in Bethesda, Maryland, the White House said. Bush\\'s last colonoscopy was in June 2002, and no abnormalities were found, White House spokesman Tony Snow said. The president\\'s doctor had recommended a repeat procedure in about five years. A colonoscopy is the most sensitive test for colon cancer, rectal cancer and polyps, small clumps of cells that can become cancerous, according to the Mayo Clinic. Small polyps may be removed during the procedure. Snow said on Friday that Bush had polyps removed during colonoscopies before becoming president. Snow himself is undergoing chemotherapy for cancer that began in his colon and spread to his liver.  Watch Snow talk about Bush\\'s procedure and his own colon cancer » . \"The president wants to encourage everybody to use surveillance,\" Snow said. The American Cancer Society recommends that people without high risk factors or symptoms begin getting screened for signs of colorectal cancer at age 50. E-mail to a friend .', 'summary': 'Five small polyps found during procedure; \"none worrisome,\" spokesman says .\\nPresident reclaims powers transferred to vice president .\\nBush undergoes routine colonoscopy at Camp David .'}\n",
      "Sample 4: {'article': '(CNN)  -- The National Football League has indefinitely suspended Atlanta Falcons quarterback Michael Vick without pay, officials with the league said Friday. NFL star Michael Vick is set to appear in court Monday. A judge will have the final say on a plea deal. Earlier, Vick admitted to participating in a dogfighting ring as part of a plea agreement with federal prosecutors in Virginia. \"Your admitted conduct was not only illegal, but also cruel and reprehensible. Your team, the NFL, and NFL fans have all been hurt by your actions,\" NFL Commissioner Roger Goodell said in a letter to Vick. Goodell said he would review the status of the suspension after the legal proceedings are over. In papers filed Friday with a federal court in Virginia, Vick also admitted that he and two co-conspirators killed dogs that did not fight well. Falcons owner Arthur Blank said Vick\\'s admissions describe actions that are \"incomprehensible and unacceptable.\" The suspension makes \"a strong statement that conduct which tarnishes the good reputation of the NFL will not be tolerated,\" he said in a statement.  Watch what led to Vick\\'s suspension » . Goodell said the Falcons could \"assert any claims or remedies\" to recover $22 million of Vick\\'s signing bonus from the 10-year, $130 million contract he signed in 2004, according to The Associated Press. Vick said he would plead guilty to one count of \"Conspiracy to Travel in Interstate Commerce in Aid of Unlawful Activities and to Sponsor a Dog in an Animal Fighting Venture\" in a plea agreement filed at U.S. District Court in Richmond, Virginia. The charge is punishable by up to five years in prison, a $250,000 fine, \"full restitution, a special assessment and 3 years of supervised release,\" the plea deal said. Federal prosecutors agreed to ask for the low end of the sentencing guidelines. \"The defendant will plead guilty because the defendant is in fact guilty of the charged offense,\" the plea agreement said. In an additional summary of facts, signed by Vick and filed with the agreement, Vick admitted buying pit bulls and the property used for training and fighting the dogs, but the statement said he did not bet on the fights or receive any of the money won. \"Most of the \\'Bad Newz Kennels\\' operations and gambling monies were provided by Vick,\" the official summary of facts said. Gambling wins were generally split among co-conspirators Tony Taylor, Quanis Phillips and sometimes Purnell Peace, it continued. \"Vick did not gamble by placing side bets on any of the fights. Vick did not receive any of the proceeds from the purses that were won by \\'Bad Newz Kennels.\\' \" Vick also agreed that \"collective efforts\" by him and two others caused the deaths of at least six dogs. Around April, Vick, Peace and Phillips tested some dogs in fighting sessions at Vick\\'s property in Virginia, the statement said. \"Peace, Phillips and Vick agreed to the killing of approximately 6-8 dogs that did not perform well in \\'testing\\' sessions at 1915 Moonlight Road and all of those dogs were killed by various methods, including hanging and drowning. \"Vick agrees and stipulates that these dogs all died as a result of the collective efforts of Peace, Phillips and Vick,\" the summary said. Peace, 35, of Virginia Beach, Virginia; Phillips, 28, of Atlanta, Georgia; and Taylor, 34, of Hampton, Virginia, already have accepted agreements to plead guilty in exchange for reduced sentences. Vick, 27, is scheduled to appear Monday in court, where he is expected to plead guilty before a judge.  See a timeline of the case against Vick » . The judge in the case will have the final say over the plea agreement. The federal case against Vick focused on the interstate conspiracy, but Vick\\'s admission that he was involved in the killing of dogs could lead to local charges, according to CNN legal analyst Jeffrey Toobin. \"It sometimes happens -- not often -- that the state will follow a federal prosecution by charging its own crimes for exactly the same behavior,\" Toobin said Friday. \"The risk for Vick is, if he makes admissions in his federal guilty plea, the state of Virginia could say, \\'Hey, look, you admitted violating Virginia state law as well. We\\'re going to introduce that against you and charge you in our court.\\' \" In the plea deal, Vick agreed to cooperate with investigators and provide all information he may have on any criminal activity and to testify if necessary. Vick also agreed to turn over any documents he has and to submit to polygraph tests. Vick agreed to \"make restitution for the full amount of the costs associated\" with the dogs that are being held by the government. \"Such costs may include, but are not limited to, all costs associated with the care of the dogs involved in that case, including if necessary, the long-term care and/or the humane euthanasia of some or all of those animals.\" Prosecutors, with the support of animal rights activists, have asked for permission to euthanize the dogs. But the dogs could serve as important evidence in the cases against Vick and his admitted co-conspirators. Judge Henry E. Hudson issued an order Thursday telling the U.S. Marshals Service to \"arrest and seize the defendant property, and use discretion and whatever means appropriate to protect and maintain said defendant property.\" Both the judge\\'s order and Vick\\'s filing refer to \"approximately\" 53 pit bull dogs. After Vick\\'s indictment last month, Goodell ordered the quarterback not to report to the Falcons training camp, and the league is reviewing the case. Blank told the NFL Network on Monday he could not speculate on Vick\\'s future as a Falcon, at least not until he had seen \"a statement of facts\" in the case.  E-mail to a friend . CNN\\'s Mike Phelan contributed to this report.', 'summary': \"NEW: NFL chief, Atlanta Falcons owner critical of Michael Vick's conduct .\\nNFL suspends Falcons quarterback indefinitely without pay .\\nVick admits funding dogfighting operation but says he did not gamble .\\nVick due in federal court Monday; future in NFL remains uncertain .\"}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the CNNContentDataset\n",
    "cnn_dataset = CNNContentDataset()\n",
    "\n",
    "\n",
    "print(f\"Number of samples in CNN dataset: {len(cnn_dataset)}\")\n",
    "# Print the first 5 samples\n",
    "for i in range(5):\n",
    "    sample = cnn_dataset[i]\n",
    "    print(f\"Sample {i}:\", sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54b300c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0a8509d310b4e998b4dee9b9d8f9ce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk.\n",
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "# Define the device for computation.\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create a baseline text-generation pipeline from the original phi-4 model.\n",
    "baseline_pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"microsoft/phi-4\",  # original model without your modifications\n",
    "    model_kwargs={\"torch_dtype\": \"auto\"},\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c28799ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a subset of 1435 samples (.5% of dataset).\n",
      "First sample type: <class 'dict'>\n",
      "First sample content: {'article': 'LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won\\'t cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don\\'t plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don\\'t think I\\'ll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he\\'ll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I\\'ll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe\\'s earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say \\'kid star goes off the rails,\\'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films.  Watch I-Reporter give her review of Potter\\'s latest » . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer\\'s \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he\\'s legally an adult: \"I just think I\\'m going to be more sort of fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.', 'summary': \"Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\\nYoung actor says he has no plans to fritter his cash away .\\nRadcliffe's earnings from first five Potter films have been held in trust fund .\"}\n"
     ]
    }
   ],
   "source": [
    "total_samples = len(cnn_dataset)\n",
    "subset_size = int(total_samples * 0.005)\n",
    "# Build a list of samples from the first subset_size indices.\n",
    "subset = [cnn_dataset[i] for i in range(subset_size)]\n",
    "print(f\"Using a subset of {subset_size} samples (.5% of dataset).\")\n",
    "\n",
    "# Print the type and content of the first sample.\n",
    "first_sample = subset[0]\n",
    "print(\"First sample type:\", type(first_sample))\n",
    "print(\"First sample content:\", first_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ddb6f40a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b02db2fff5f442659f0ee8a6517b680d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating samples:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: {'article': 'WASHINGTON (CNN) -- As he awaits a crucial progress report on Iraq, President Bush will try to put a twist on comparisons of the war to Vietnam by invoking the historical lessons of that conflict to argue against pulling out. President Bush pauses Tuesday during a news conference at the  North American Leaders summit in Canada. On Wednesday in Kansas City, Missouri, Bush will tell members of the Veterans of Foreign Wars that \"then, as now, people argued that the real problem was America\\'s presence and that if we would just withdraw, the killing would end,\" according to speech excerpts released Tuesday by the White House. \"Three decades later, there is a legitimate debate about how we got into the Vietnam War and how we left,\" Bush will say. \"Whatever your position in that debate, one unmistakable legacy of Vietnam is that the price of America\\'s withdrawal was paid by millions of innocent citizens, whose agonies would add to our vocabulary new terms like \\'boat people,\\' \\'re-education camps\\' and \\'killing fields,\\' \" the president will say. The president will also make the argument that withdrawing from Vietnam emboldened today\\'s terrorists by compromising U.S. credibility, citing a quote from al Qaeda leader Osama bin Laden that the American people would rise against the Iraq war the same way they rose against the war in Vietnam, according to the excerpts. \"Here at home, some can argue our withdrawal from Vietnam carried no price to American credibility, but the terrorists see things differently,\" Bush will say. On Tuesday, Democratic Senate Majority Leader Harry Reid said, \"President Bush\\'s attempt to compare the war in Iraq to past military conflicts in East Asia ignores the fundamental difference between the two. Our nation was misled by the Bush Administration in an effort to gain support for the invasion of Iraq under false pretenses, leading to one of the worst foreign policy blunders in our history. \"While the President continues to stay-the-course with his failed strategy in Iraq, paid for by the taxpayers, American lives are being lost and there is still no political solution within the Iraqi government. It is time to change direction in Iraq, and Congress will again work to do so in the fall.\" The White House is billing the speech, along with another address next week to the American Legion, as an effort to \"provide broader context\" for the debate over the upcoming Iraq progress report by Gen. David Petraeus, the top U.S. military commander, and Ryan Crocker, the U.S. ambassador in Baghdad. President Bush has frequently asked lawmakers -- and the American people -- to withhold judgment on his troop \"surge\" in Iraq until the report comes out in September.  Watch Bush criticize the Iraqi government » . It is being closely watched on Capitol Hill, particularly by Republicans nervous about the political fallout from an increasingly unpopular war. Earlier this month, Defense Secretary Robert Gates said he would wait for the report before deciding when a drawdown of the 160,000 U.S. troops in Iraq might begin. Bush\\'s speeches Wednesday and next week are the latest in a series of attempts by the White House to try to reframe the debate over Iraq, as public support for the war continues to sag. A recent CNN/Opinion Research Corporation poll found that almost two-thirds of Americans -- 64 percent -- now oppose the Iraq war, and 72 percent say that even if Petraeus reports progress, it won\\'t change their opinion. The poll also found a great deal of skepticism about the report; 53 percent said they do not trust Petraeus to give an accurate assessment of the situation in Iraq. In addition to his analogy to Vietnam, Bush in Wednesday\\'s speech will invoke other historical comparisons from Asia, including the U.S. defeat and occupation of Japan after World War II and the Korean War in the 1950s, according to the excerpts. \"In the aftermath of Japan\\'s surrender, many thought it naive to help the Japanese transform themselves into a democracy. Then, as now, the critics argued that some people were simply not fit for freedom,\" Bush will say. \"Today, in defiance of the critics, Japan ... stands as one of the world\\'s great free societies.\" Speaking about the Korean War, Bush will note that at the time \"critics argued that the war was futile, that we never should have sent our troops in, or that America\\'s intervention was divisive here at home.\" \"While it is true that the Korean War had its share of challenges, America never broke its word,\" Bush will say. \"Without America\\'s intervention during the war, and our willingness to stick with the South Koreans after the war, millions of South Koreans would now be living under a brutal and repressive regime.\" E-mail to a friend .', 'summary': \"President Bush to address the Veterans of Foreign Wars on Wednesday .\\nBush to say that withdrawing from Vietnam emboldened today's terrorists .\\nSpeech will be latest White House attempt to try to reframe the debate over Iraq .\"}\n",
      "Sample: {'article': 'LONDON, England (CNN) -- A chronology of bombings and attempted bomb attacks in the mainland UK since the 1970s: . Police close off streets around Haymarket, in London\\'s busy theater district. June 29, 2007: Police defuse a bomb consisting of 200 liters of fuel, gas cylinders and nails found in an abandoned car in Haymarket, central London. A second car packed with gas and nails was later found to have been parked just a few hundred yards from the first, before it was towed away by traffic wardens in the early hours of Friday for violating parking restrictions. Police say two vehicles clearly linked. July 21, 2005: Two weeks after the deadly 7/7 bombings, four men are alleged to have attempted to carry out a second wave of attacks against London\\'s transport network at three London underground stations and aboard a bus. But their alleged rucksack bombs fail to explode. July 7, 2005: Four suicide bombers detonate themselves aboard three underground trains and a bus in a morning rush hour attack against London\\'s transport network, killing 52 people and injuring around 700 more. Al Qaeda claims responsibility in a video statement. August 2004: Anti-terrorist police disrupt a plot by Islamic militants to blow up targets including the Ministry of Sound nightclub and the Bluewater shopping center in southeast England using explosives packed into limousines and large vehicles. Seven men are convicted in May 2007 and sentenced to up to 26 years in prison. March 2001: A car bomb explodes outside the BBC\\'s London headquarters, wounding one man. Police blame the Real IRA, a republican splinter group opposed to the IRA\\'s cease fire. April 1999: Three people die when a nail bomb explodes in the Admiral Duncan pub in London\\'s gay district -- the third in a spate of series of nail bomb attacks also targeting immigrant areas of the city that left dozens injured. A 23-year-old self-declared \"Nazi\", David Copeland, is sentenced to six life terms. June 1996: A massive IRA bomb explodes in a shopping center in central Manchester, injuring more than 200 people. February 1996: Two people die as IRA terrorists detonate a bomb in London\\'s Docklands area, causing damage estimated at around $170m and ending the group\\'s 17-month cease fire. April 1993: An IRA truck bomb devastates part of London\\'s financial district, killing one and wounding 44. March 1993: Two boys aged three and 12 are killed and dozens are injured by two bombs left in litter bins in Warrington, northern England. The IRA admits planting the bombs. April 1992: A huge IRA car bomb in London\\'s financial district kills three people and wounds 91. February 1991: IRA terrorists launch a mortar attack at Prime Minister John Major\\'s Downing Street offices. No-one is injured. September 1989: Eleven people die and 22 are wounded when an IRA bomb explodes at a Royal Marine music school in Deal, southern England. December 1988: A Pan Am airliner explodes over the Scottish town of Lockerbie, killing 259 aboard and 11 people on the ground. Libyan agent Abdel Basset al-Megrahi, convicted of the attack in 2001, was this week granted the right to mount a fresh appeal. (Read about Lockerbie bomber) October 1984: Five people die in an IRA bomb attack on a hotel in Brighton, southern England, where Prime Minister Margaret Thatcher and her cabinet are staying for the Conservative Party\\'s annual conference. December 1983: An IRA bomb at London\\'s Harrods department store kills six people. July 1982: Two IRA bomb attacks on soldiers in London\\'s parks kill 11 people and wound 50. October-November 1974: A wave of IRA bombs in British pubs in Birmingham and Guildford kill 28 people and wound more than 200. February 1974: A coach carrying soldiers and families in northern England is bombed by the IRA, killing 12 and wounding 14. E-mail to a friend .', 'summary': 'Two cars loaded with gasoline and nails found abandoned in London Friday .\\n52 people killed on July 7, 2005 after bombs exploded on London bus, trains .\\nBritish capital wracked by violence by the IRA for years .'}\n",
      "Sample: {'article': 'BREMEN, Germany -- Carlos Alberto, who scored in FC Porto\\'s Champions League final victory against Monaco in 2004, has joined Bundesliga club Werder Bremen for a club record fee of  7.8 million euros ($10.7 million). Carlos Alberto enjoyed success at FC Porto under Jose Mourinho. \"I\\'m here to win titles with Werder,\" the 22-year-old said after his first training session with his new club. \"I like Bremen and would only have wanted to come here.\" Carlos Alberto started his career with Fluminense, and helped them to lift the Campeonato Carioca in 2002. In January 2004 he moved on to FC Porto, who were coached by José Mourinho, and the club won the Portuguese title as well as the Champions League. Early in 2005, he moved to Corinthians, where he impressed as they won the Brasileirão,but in 2006 Corinthians had a poor season and Carlos Alberto found himself at odds with manager, Emerson Leão. Their poor relationship came to a climax at a Copa Sul-Americana game against Club Atlético Lanús, and Carlos Alberto declared that he would not play for Corinthians again while Leão remained as manager. Since January this year he has been on loan with his first club Fluminense. Bundesliga champions VfB Stuttgart said on Sunday that they would sign a loan agreement with Real Zaragoza on Monday for Ewerthon, the third top Brazilian player to join the German league in three days. A VfB spokesman said Ewerthon, who played in the Bundesliga for Borussia Dortmund from 2001 to 2005, was expected to join the club for their pre-season training in Austria on Monday. On Friday, Ailton returned to Germany where he was the league\\'s top scorer in 2004, signing a one-year deal with Duisburg on a transfer from Red Star Belgrade. E-mail to a friend .', 'summary': 'Werder Bremen pay a club record $10.7 million for Carlos Alberto .\\nThe Brazilian midfielder won the Champions League with FC Porto in 2004 .\\nSince January he has been on loan with his first club, Fluminense .'}\n",
      "Sample: {'article': 'WASHINGTON (CNN) -- Vice President Dick Cheney will serve as acting president briefly Saturday while President Bush is anesthetized for a routine colonoscopy, White House spokesman Tony Snow said Friday. Bush is scheduled to have the medical procedure, expected to take about 2 1/2 hours, at the presidential retreat at Camp David, Maryland, Snow said. Bush\\'s last colonoscopy was in June 2002, and no abnormalities were found, Snow said. The president\\'s doctor had recommended a repeat procedure in about five years. The procedure will be supervised by Dr. Richard Tubb and conducted by a multidisciplinary team from the National Naval Medical Center in Bethesda, Maryland, Snow said. A colonoscopy is the most sensitive test for colon cancer, rectal cancer and polyps, small clumps of cells that can become cancerous, according to the Mayo Clinic. Small polyps may be removed during the procedure. Snow said that was the case when Bush had colonoscopies before becoming president. Snow himself is undergoing chemotherapy for cancer that began in his colon and spread to his liver. Snow told reporters he had a chemo session scheduled later Friday.  Watch Snow talk about Bush\\'s procedure and his own colon cancer » . \"The president wants to encourage everybody to use surveillance,\" Snow said. The American Cancer Society recommends that people without high-risk factors or symptoms begin getting screened for signs of colorectal cancer at age 50. E-mail to a friend .', 'summary': \"President Bush will have a routine colonoscopy Saturday .\\nWhile he's anesthetized, his powers will be transferred to the vice president .\\nBush had last colonoscopy in 2002, which found no problems .\"}\n",
      "Sample: {'article': 'SAN FRANCISCO, California (CNN)  -- A magnitude 4.2 earthquake shook the San Francisco area Friday at 4:42 a.m. PT (7:42 a.m. ET), the U.S. Geological Survey reported. The quake left about 2,000 customers without power, said David Eisenhower, a spokesman for Pacific Gas and Light. Under the USGS classification, a magnitude 4.2 earthquake is considered \"light,\" which it says usually causes minimal damage. \"We had quite a spike in calls, mostly calls of inquiry, none of any injury, none of any damage that was reported,\" said Capt. Al Casciato of the San Francisco police. \"It was fairly mild.\"  Watch police describe concerned calls immediately after the quake » . The quake was centered about two miles east-northeast of Oakland, at a depth of 3.6 miles, the USGS said. Oakland is just east of San Francisco, across San Francisco Bay. An Oakland police dispatcher told CNN the quake set off alarms at people\\'s homes. The shaking lasted about 50 seconds, said CNN meteorologist Chad Myers. According to the USGS, magnitude 4.2 quakes are felt indoors and may break dishes and windows and overturn unstable objects. Pendulum clocks may stop. E-mail to a friend .', 'summary': '2,000 customers without electricity, power company says .\\nMagnitude 4.2 quake set off home alarms, says Oakland police dispatcher .\\n\"It was fairly mild,\" police say, no immediate reports of injuries, damage .\\nIt was centered two miles east-northeast of Oakland, about 3.6 miles deep .'}\n",
      "Sample: {'article': 'WASHINGTON (CNN) -- There is \"no remaining hope\" of finding six men trapped for almost a month in a Utah coal mine alive, a federal official said Saturday. Isaac Arellano holds a candle and sings during a fundraiser for miners Tuesday in Price, Utah. \"Over the past 25 days, the Mine Safety and Health Administration has exhausted all known options in our attempt to reach the six miners,\" Richard Stickler, head of the agency, said in a statement. \"The thoughts and prayers of the dedicated professionals at MSHA are with the families.\" Sympathy for the failed efforts also came Saturday from the White House. \"Last night, a difficult decision was made to end the search,\" President Bush said in a statement. \"Laura and I are deeply saddened by this tragedy and continue to pray for the families of these men.\" Labor Secretary Elaine Chao called the ordeal \"heartbreaking.\" \"The grueling around-the-clock rescue operation that claimed three lives and injured six others has also taken a tremendous toll on the many brave rescuers and the local community, and our thoughts and prayers are with them all,\" Chao said in a statement. After drilling seven holes into mine tunnels from the mountaintop above, there has been no sign of the miners -- and microphones have picked up no sound from the men.  See a timeline of rescue efforts » . Tests showed underground oxygen levels were too low to sustain human life. \"We basically told the families that at this point in time we\\'ve run out of options,\" Stickler said at a news conference late Friday. \"We\\'ve consulted with the people that we have here, we\\'ve consulted with the technical support in Pittsburgh and we\\'ve consulted with private consultants in terms of where we can go,\" he said. \"And basically, through all the information we\\'ve gleaned over the past nearly four weeks in terms of the conditions we found, in terms of the air readings we found down there and ... everything else, we just don\\'t know where else we can put a hole to get any other information.\"  See photos of the rescue mission » . There were no public statements Saturday from Bob Murray, president and CEO of Murray Mining, co-owner of the Crandall Canyon Mine, who was the outspoken face of the rescue operation for the first three weeks, then largely disappeared from public view. Federal officials became the spokesmen. No one from Murray Mining was present at Friday\\'s news conference. \"They are done. It\\'s finished,\" the attorney for the families said, according to the Saturday edition of The Salt Lake Tribune. \"It\\'s a hard and bitter pill for our families, and there were quite a few tears shed,\" the newspaper quotes Colin King as saying. The men were trapped during a collapse on August 6, and it is not known whether they survived the cave-in. Efforts to reach them were suspended 10 days later when two rescuers and a federal mining official were killed, and six people were injured in a second collapse as they tried to tunnel horizontally toward the area where the men had been working. Murray said last week that the search effort would stop if no signs of life were found at the sixth hole. Under pressure from the families, however, he agreed to try one more time. Families wanted officials to drill a hole large enough to send down a rescue capsule. The effort to lower the robotic device down a seventh hole had been called \"a long shot\" by an official. MSHA\\'s Stickler said that hole was drilled into the Crandall Canyon Mine on Thursday, but there were problems with a robotic camera that teams were trying to lower into it. Work resumed Friday, this time at the fourth hole, but the camera could only descend about 7 feet, he said. \"Basically, what it saw was really not that much. There was quite a bit of mud in there, water coming down the hole. It really couldn\\'t go any farther than seven feet,\" he said of the latest try. In addition, the roof was sagging. \"The families asked many, many questions and we answered them all the best we could, basically coming to the conclusion that we had run out of options.\" Murray said last Saturday he has already filed paperwork with federal regulators to permanently close and seal the Crandall Canyon mine. \"I will never come back to that evil mountain,\" he said. Friends and family have identified the six missing miners as Luis Hernandez, Manuel Sanchez, Kerry Allred, Carlos Payan, Brandon Phillips and Don Erickson. E-mail to a friend .', 'summary': \"NEW: President Bush says he and first lady are deeply saddened by the tragedy .\\nMine Safety and Health Administration chief: We've run out of options.\\nThe six men have been trapped underground since August 6 .\\nSeven bore holes drilled into the mountain have found no signs of life .\"}\n",
      "Sample: {'article': '(CNN) -- At least 14 people were killed and 60 others wounded Thursday when a bomb ripped through a crowd waiting to see Algeria\\'s president in Batna, east of the capital of Algiers, the Algerie Presse Service reported. A wounded person gets first aid shortly after Thursday\\'s attack in Batna, Algeria. The explosion occurred at 5 p.m. about 20 meters (65 feet) from a mosque in Batna, a town about 450 kilometers (280 miles) east of Algiers, security officials in Batna told the state-run news agency. The bomb went off 15 minutes before the expected arrival of President Abdel-Aziz Bouteflika. It wasn\\'t clear if the bomb was caused by a suicide bomber or if it was planted, the officials said. Later Thursday, Algeria\\'s Interior Minister Noureddine Yazid Zerhouni said \"a suspect person who was among the crowd attempted to go beyond the security cordon,\" but the person escaped \"immediately after the bomb exploded,\" the press service reported. Bouteflika made his visit to Batna as planned, adding a stop at a hospital to visit the wounded before he returned to the capital. There was no immediate claim of responsibility for the bombing. Algeria faces a continuing Islamic insurgency, according to the CIA. In July, 33 people were killed in apparent suicide bombings in Algiers that were claimed by an al Qaeda-affiliated group. Bouteflika said terrorist acts have nothing in common with the noble values of Islam, the press service reported. E-mail to a friend . CNN\\'s Mohammed Tawfeeq contributed to this report.', 'summary': \"Bomb victims waiting for presidential visit .\\nBlast went off 15 minutes before president's arrival .\\nAlgeria faces Islamic insurgency .\\nAl Qaeda-affiliated group claimed July attacks .\"}\n",
      "Sample: {'article': '(CNN) -- Football superstar, celebrity, fashion icon, multimillion-dollar heartthrob. Now, David Beckham is headed for the Hollywood Hills as he takes his game to U.S. Major League Soccer. CNN looks at how Bekham fulfilled his dream of playing for Manchester United, and his time playing for England. The world\\'s famous footballer has begun a five-year contract with the Los Angeles Galaxy team, and on Friday Beckham will meet the press and reveal his new shirt number. This week, we take an in depth look at the life and times of Beckham, as CNN\\'s very own \"Becks,\" Becky Anderson, sets out to examine what makes the man tick -- as footballer, fashion icon and global phenomenon. It\\'s a long way from the streets of east London to the Hollywood Hills and Becky charts Beckham\\'s incredible rise to football stardom, a journey that has seen his skills grace the greatest stages in world soccer. She goes in pursuit of the current hottest property on the sports/celebrity circuit in the U.S. and along the way explores exactly what\\'s behind the man with the golden boot. CNN will look back at the life of Beckham, the wonderfully talented youngster who fulfilled his dream of playing for Manchester United, his marriage to pop star Victoria, and the trials and tribulations of playing for England. We\\'ll look at the highs (scoring against Greece), the lows (being sent off during the World Cup), the Man. U departure for the Galacticos of Madrid -- and now the Home Depot stadium in L.A. We\\'ll ask how Beckham and his family will adapt to life in Los Angeles -- the people, the places to see and be seen and the celebrity endorsement. Beckham is no stranger to exposure. He has teamed with Reggie Bush in an Adidas commercial, is the face of Motorola, is the face on a PlayStation game and doesn\\'t need fashion tips as he has his own international clothing line. But what does the star couple need to do to become an accepted part of Tinseltown\\'s glitterati? The road to major league football in the U.S.A. is a well-worn route for some of the world\\'s greatest players. We talk to some of the former greats who came before him and examine what impact these overseas stars had on U.S. soccer and look at what is different now. We also get a rare glimpse inside the David Beckham academy in L.A, find out what drives the kids and who are their heroes. The perception that in the U.S.A. soccer is a \"game for girls\" after the teenage years is changing. More and more young kids are choosing the European game over the traditional U.S. sports. E-mail to a friend .', 'summary': 'Beckham has agreed to a five-year contract with Los Angeles Galaxy .\\nNew contract took effect July 1, 2007 .\\nFormer English captain to meet press, unveil new shirt number Friday .\\nCNN to look at Beckham as footballer, fashion icon and global phenomenon .'}\n",
      "Sample: {'article': '(CNN) -- A virus found in healthy Australian honey bees may be playing a role in the collapse of honey bee colonies across the United States, researchers reported Thursday. Honey bees walk on a moveable comb hive at the Bee Research Laboratory, in Beltsville, Maryland. Colony collapse disorder has killed millions of bees -- up to 90 percent of colonies in some U.S. beekeeping operations -- imperiling the crops largely dependent upon bees for pollination, such as oranges, blueberries, apples and almonds. The U.S. Department of Agriculture says honey bees are responsible for pollinating $15 billion worth of crops each year in the United States. More than 90 fruits and vegetables worldwide depend on them for pollination. Signs of colony collapse disorder were first reported in the United States in 2004, the same year American beekeepers started importing bees from Australia. The disorder is marked by hives left with a queen, a few newly hatched adults and plenty of food, but the worker bees responsible for pollination gone. The virus identified in the healthy Australian bees is  Israeli Acute Paralysis Virus (IAPV) -- named that because it was discovered by Hebrew University researchers. Although worker bees in colony collapse disorder vanish, bees infected with IAPV die close to the hive, after developing shivering wings and paralysis. For some reason, the Australian bees seem to be resistant to IAPV and do not come down with symptoms. Scientists used genetic analyses of bees collected over the past three years and found that IAPV was present in bees that had come from colony collapse disorder hives 96 percent of the time. But the study released Thursday on the Science Express Web site, operated by the journal Science, cautioned that collapse disorder is likely caused by several factors. \"This research give us a very good lead to follow, but we do not believe IAPV is acting alone,\" said Jeffery S. Pettis of the U.S. Department of Agriculture\\'s Bee Research Laboratory and a co-author of the study. \"Other stressors on the colony are likely involved.\" This could explain why bees in Australia may be resistant to colony collapse. \"There are no cases ... in Australia at all,\" entomologist Dave Britton of the Australian Museum told the Sydney Morning Herald last month. \"It is a Northern Hemisphere phenomenon.\" Bee ecology expert and University of Florida professor Jamie Ellis said earlier this year that genetic weakness bred into bees over time, pathogens spread by parasites and the effects of pesticides and pollutants might be other factors. Researchers also say varroa mites affect all hives on the U.S. mainland but are not found in Australia. University of Georgia bee researcher Keith S. Delaplane said Thursday the study offers a warning -- and hope. \"One nagging problem has been a general inability to treat or vaccinate bees against viruses of any kind,\" said Delaplane, who has been trying to breed bees resistant to the varroa mite. \"But in the case of IAPV, there is evidence that some bees carry genetic resistance to the disorder. This is yet one more argument for beekeepers to use honey bee stocks that are genetically disease- and pest-resistant.\" Bee researchers will now look for stresses that may combine to kill bees. \"The next step is to ascertain whether IAPV, alone or in concert with other factors, can induce CCD [colony collapse disorder] in healthy bees,\" said Ian Lipkin, director of the Center for Infection and Immunity at Columbia University Mailman School of Public Health. Besides the Columbia and USDA researchers, others involved in the study released Thursday include researchers from Pennsylvania State University, the Pennsylvania Department of Agriculture, the University of Arizona and 454 Life Sciences. E-mail to a friend .', 'summary': 'Colony collapse disorder has killed millions of bees .\\nScientists suspect a virus may combine with other factors to collapse colonies .\\nDisorder first cropped up in 2004, as bees were imported from Australia .\\n$15 billion in U.S. crops each year dependent on bees for pollination .'}\n",
      "Sample: {'article': 'LONDON, England -- Savers at a leading UK mortgage bank lined up for a second day to empty their accounts Saturday, a day after the lender was bailed out by the Bank of England after heavily slashing profit forecasts. Fearful customers line up to withdraw cash from a Northern Rock branch in southeast London on Friday. Long lines formed before counters opened at the Northern Rock building society, one of the UK\\'s top five lenders, as worried customers ignored reassurances from the bank and the government. Customers are believed to have already withdrawn about £1 billion ($2 billion) since the bank\\'s woes were revealed, prompting speculation that the global credit crunch made raising funds through commercial borrowing difficult. Shares in Northern Rock dropped up to 30 percent in Friday trading, with problems spilling over the European banking sector . The British Bankers\\' Association has urged customers to \"calm down,\" according to the UK Press Association. It said: \"Northern Rock is a sound and safe bank and there is absolutely no reason for either mortgage customers or savers to worry.\" Meanwhile, finance minister Alistair Darling said the Bank of England had stepped in \"to create a stable banking system\". He said: \"People can use their accounts in the usual way, they can carry on making their mortgage payments in the usual way. Northern Rock will be able to carry on its business.\" Northern Rock chief executive Adam Applegarth said yesterday that the bank had yet to draw on the emergency cash, which he called \"a backdrop in case we need to use it\", according to PA. E-mail to a friend .', 'summary': 'Savers at leading UK mortgage bank lined up to empty their accounts .\\nNorthern Rock was bailed out by the Bank of England a day earlier .\\nReassurances that banks was safe have gone unheeded by many .'}\n"
     ]
    }
   ],
   "source": [
    "baseline_predictions = []\n",
    "modified_predictions = []\n",
    "references = []\n",
    "\n",
    "# Loop over each sample in the subset with a progress bar.\n",
    "for sample in tqdm(subset[10:20], desc=\"Evaluating samples\"):\n",
    "    print(\"Sample:\", sample)\n",
    "    article = sample[\"article\"]\n",
    "    reference_summary = sample[\"summary\"]\n",
    "\n",
    "    # print(\"Baseline out...\")\n",
    "    # Generate summary using the baseline (original phi-4) model.\n",
    "    baseline_out = baseline_pipeline(\n",
    "        article,\n",
    "        max_new_tokens=10,\n",
    "        min_new_tokens=0,\n",
    "        # do_sample=False,\n",
    "        # truncation=True\n",
    "    )\n",
    "    # print(\"Baseline out:\", baseline_out)\n",
    "    baseline_summary = baseline_out[0].get(\"summary_text\", baseline_out[0].get(\"generated_text\", \"\"))\n",
    "\n",
    "    # Generate summary using the modified (fine-tuned) model.\n",
    "    # print(\"Modified out...\")\n",
    "    modified_out = pipeline(\n",
    "        article,\n",
    "        max_new_tokens=10,\n",
    "        min_new_tokens=0,\n",
    "        # do_sample=False,\n",
    "        # truncation=True\n",
    "    )\n",
    "    modified_summary = modified_out[0].get(\"summary_text\", modified_out[0].get(\"generated_text\", \"\"))\n",
    "\n",
    "    baseline_predictions.append(baseline_summary)\n",
    "    modified_predictions.append(modified_summary)\n",
    "    references.append(reference_summary)\n",
    "\n",
    "# Load the ROUGE evaluation metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e31eedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump the results to JSON so we can use them later.\n",
    "import json\n",
    "with open(\"rouge_results.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"baseline_predictions\": baseline_predictions,\n",
    "        \"modified_predictions\": modified_predictions,\n",
    "        \"references\": references\n",
    "    }, f, indent=4)\n",
    "# Evaluate the baseline model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ec28f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K  \u001b[31m×\u001b[0m No solution found when resolving dependencies for split                         \u001b[0m\n",
      "  \u001b[31m│\u001b[0m (python_full_version == '3.11.*'):\n",
      "\u001b[31m  ╰─▶ \u001b[0mBecause there are no versions of absl and your project depends on absl,\n",
      "\u001b[31m      \u001b[0mwe can conclude that your project's requirements are unsatisfiable.\n",
      "\u001b[36m  help: \u001b[0mIf this is intentional, run `uv add --frozen` to skip the lock and\n",
      "        sync steps.\n"
     ]
    }
   ],
   "source": [
    "!uv add absl rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7001b72c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b42e66c8",
   "metadata": {},
   "source": [
    "# Steering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3373af14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard-code the style latent and see how it affects the output.\n",
    "# In other words, we want to see if we can get the model to generate a specific style of text.\n",
    "# To do this, we need to set the style latent to a specific value, do a forward pass through the model,\n",
    "# and keep the latent fixed (i.e., don't allow it to be updated at each step).\n",
    "\n",
    "def generate_with_fixed_latent(\n",
    "    model: nn.Module,\n",
    "    tokenizer,\n",
    "    text: str,\n",
    "    style_latent: torch.Tensor,\n",
    "    layer_idx=21,\n",
    "    max_new_tokens=130,\n",
    "    min_new_tokens=30,\n",
    "    do_sample=False,\n",
    "    truncation=True,\n",
    "    **kwargs\n",
    "):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=truncation,\n",
    "        max_length=512,\n",
    "    ).to(device)\n",
    "    # Get the hidden states from the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states[layer_idx]\n",
    "        x = hidden_states.mean(dim=1).to(torch.float32).to(device)\n",
    "    # Get the style latent from the bottleneck\n",
    "    style_latent = style_latent.to(device)\n",
    "    # Get the logits from the model\n",
    "    style_layer = model.model.layers[layer_idx]\n",
    "    logits = style_layer.bottleneck.up(style_latent)\n",
    "    # Get the logits from the model\n",
    "    logits = style_layer(x)\n",
    "    # Generate the output text\n",
    "    generated_ids = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        min_new_tokens=min_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        logits_processor=logits,\n",
    "        **kwargs\n",
    "    )\n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return generated_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec7824ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m generated_text = \u001b[43mgenerate_with_fixed_latent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mYour input text here\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstyle_latent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Example style latent\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mgenerate_with_fixed_latent\u001b[39m\u001b[34m(model, tokenizer, text, style_latent, layer_idx, max_new_tokens, min_new_tokens, do_sample, truncation, **kwargs)\u001b[39m\n\u001b[32m     35\u001b[39m logits = style_layer.bottleneck.up(style_latent)\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Get the logits from the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m logits = \u001b[43mstyle_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Generate the output text\u001b[39;00m\n\u001b[32m     39\u001b[39m generated_ids = model.generate(\n\u001b[32m     40\u001b[39m     inputs.input_ids,\n\u001b[32m     41\u001b[39m     max_new_tokens=max_new_tokens,\n\u001b[32m   (...)\u001b[39m\u001b[32m     45\u001b[39m     **kwargs\n\u001b[32m     46\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/school/S25/ATDL/llm_style/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/school/S25/ATDL/llm_style/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mResidualBottleneckWrapper.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m     30\u001b[39m bottleneck_out = \u001b[38;5;28mself\u001b[39m.bottleneck(x)\n\u001b[32m     31\u001b[39m x_modified = x + bottleneck_out\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_modified\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/school/S25/ATDL/llm_style/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/school/S25/ATDL/llm_style/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/school/S25/ATDL/llm_style/.venv/lib/python3.11/site-packages/transformers/models/phi3/modeling_phi3.py:303\u001b[39m, in \u001b[36mPhi3DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    300\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    302\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m hidden_states, self_attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    314\u001b[39m hidden_states = residual + \u001b[38;5;28mself\u001b[39m.resid_attn_dropout(hidden_states)  \u001b[38;5;66;03m# main diff with Llama\u001b[39;00m\n\u001b[32m    316\u001b[39m residual = hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/school/S25/ATDL/llm_style/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/school/S25/ATDL/llm_style/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/school/S25/ATDL/llm_style/.venv/lib/python3.11/site-packages/transformers/models/phi3/modeling_phi3.py:197\u001b[39m, in \u001b[36mPhi3Attention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[39m\n\u001b[32m    194\u001b[39m key_states = key_states.view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    195\u001b[39m value_states = value_states.view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m cos, sin = position_embeddings\n\u001b[32m    198\u001b[39m query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    201\u001b[39m     \u001b[38;5;66;03m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "generated_text = generate_with_fixed_latent(\n",
    "    model=pipeline.model,\n",
    "    tokenizer=pipeline.tokenizer,\n",
    "    text=\"Your input text here\",\n",
    "    style_latent=torch.tensor([1.0, 0.0, 0.0, 0.0, 0.0, 0.0]),  # Example style latent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dfa793",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
